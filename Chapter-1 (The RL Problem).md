<b>Exercise 1.1: Self-Play</b> Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?  
<b>Solution:</b> The agent would continually get better at playing the game by understanding its opponent's (i.e. itself) style of playing and coming up with moves that could outplay it. The agent would improve over time by challenging the improved self. After a lot of playing against itself and improving well enough, it would be difficult to come up with more learning and imrpove further. Hence, the agent would keep beating around the optimal gameplay trying to beat the best moves it has learnt until then - which wouldn't really be possible.  
  
<b>Exercise 1.2: Symmetries</b> Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what ways would this improve it? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?  
<b>Solution:</b> We could define an identity for all the symetric states and assign the same value to each of them. This would help the agent by mitigating some of the explorations that would be redundant. Now, assuming that the opponent doesn't treat the symmetric states in the same way, we might want not want to assign the same values to each of them. We could rather let the agent learn the peculiarities/biases in the opponent's play. Once it figures out a symmetric state where the opponent plays in a sub-optimal way comapred to its peer states, we could assign a higher value to that state and exploit the opponent's bias.  
  
<b>Exercise 1.3: Greedy Play</b> Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Would it learn to play better, or worse, than a nongreedy player? What problems might occur?  
<b>Solution:</b> It would have played equally or worse by playing greedy. The player would be most tempted by the short term rewards and would possibly forgo the bigger rewards in the long run. Whereas RL is about maximising the long term/end rewards even if the player has to compromise on the immediate reward.  
  
<b>Exercise 1.4: Learning from Exploration</b> Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?  
<b>Solution:</b> The set of probabilities we compute by learning from exploratory moves would be biased ones with higher probability, whereas the ones we compute by not learning from the exploratory moves would be unbiased. This is so because we reach new states - say worse than the best successor state known already,  upon exploration whose values aren't computed already, and we use this first naive estimate to update the values of the previous states - which would lower their value estimates. However, assuming that the exploratory moves would lead us to a state that is better than the best known "next state", we would want to learn from such moves. Given that the estimates for the values for the first case are biased, they would perform at least as bad as the unbiased estimates. However, I feel nothing can concretely be said about which would result in more wins.  
  
<b>Exercise 1.5: Other Improvements</b> Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?  
<b>Solution:</b> As the player adapts and improves with experience, the older updates could be decayed exponentially because the player would already have learnt to play in the older scenarios. 
