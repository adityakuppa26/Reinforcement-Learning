<b>Exercise 1.1: Self-Play</b> Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?  
Solution: The agent would continually get better at playing the game by understanding its opponent's (i.e. itself) style of playing and coming up with moves that could outplay it. The agent would improve over time by challenging the improved self. After a lot of playing against itself and improving well enough, it would be difficult to come up with more learning and imrpove further. Hence, the agent would keep beating around the optimal gameplay trying to beat the best moves it has learnt until then - which wouldn't really be possible.  
  
<b>Exercise 1.2: Symmetries</b> Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what ways would this improve it? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?  
Solution: We could define an identity for all the symetric states and assign the same value to each of them. This would help the agent by mitigating some of the explorations that would be redundant. Now, assuming that the opponent doesn't treat the symmetric states in the same way, we might want not want to assign the same values to each of them. We could rather let the agent learn the peculiarities/biases in the opponent's play. Once it figures out a symmetric state where the opponent plays in a sub-optimal way comapred to its peer states, we could assign a higher value to that state and exploit the opponent's bias.  

